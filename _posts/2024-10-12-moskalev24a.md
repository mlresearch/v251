---
title: SE(3)-Hyena Operator for Scalable Equivariant Learning
abstract: Modeling global geometric context while maintaining equivariance is crucial
  for accurate predictions in many fields such as biology, chemistry, or vision. Yet,
  this is challenging due to the computational demands of processing high-dimensional
  data at scale. Existing approaches such as equivariant self-attention or distance-based
  message passing, suffer from quadratic complexity with respect to sequence length,
  while localized methods sacrifice global information. Inspired by the recent success
  of state-space and long-convolutional models, in this work, we introduce SE(3)-Hyena
  operator, an equivariant long-convolutional model based on the Hyena operator. The
  SE(3)-Hyena captures global geometric context at sub-quadratic complexity while
  maintaining equivariance to rotations and translations. Evaluated on equivariant
  associative recall and n-body modeling, SE(3)-Hyena matches or outperforms equivariant
  self-attention while requiring significantly less memory and computational resources
  for long sequences. Our model processes the geometric context of 20k tokens x3.5
  times faster than the equivariant transformer and allows x175 longer a context within
  the same memory budget.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: moskalev24a
month: 0
tex_title: SE(3)-Hyena Operator for Scalable Equivariant Learning
firstpage: 7
lastpage: 19
page: 7-19
order: 7
cycles: false
bibtex_author: Moskalev, Artem and Prakash, Mangal and Liao, Rui and Mansi, Tommaso
author:
- given: Artem
  family: Moskalev
- given: Mangal
  family: Prakash
- given: Rui
  family: Liao
- given: Tommaso
  family: Mansi
date: 2024-10-12
address:
container-title: Proceedings of the Geometry-grounded Representation Learning and
  Generative Modeling Workshop (GRaM)
volume: '251'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 10
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v251/main/assets/moskalev24a/moskalev24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
