---
title: Decoder ensembling for learned latent geometries
abstract: Latent space geometry provides a rigorous and empirically valuable framework
  for interacting with the latent variables of deep generative models. This approach
  reinterprets Euclidean latent spaces as Riemannian through a pull-back metric, allowing
  for a standard differential geometric analysis of the latent space. Unfortunately,
  data manifolds are generally compact and easily disconnected or filled with holes,
  suggesting a topological mismatch to the Euclidean latent space. The most established
  solution to this mismatch is to let uncertainty be a proxy for topology, but in
  neural network models, this is often realized through crude heuristics that lack
  principle and generally do not scale to high-dimensional representations. We propose
  using ensembles of decoders to capture model uncertainty and show how to easily
  compute geodesics on the associated expected manifold. Empirically, we find this
  simple and reliable, thereby coming one step closer to easy-to-use latent geometries.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: syrota24a
month: 0
tex_title: Decoder ensembling for learned latent geometries
firstpage: 277
lastpage: 285
page: 277-285
order: 277
cycles: false
bibtex_author: Syrota, Stas and Moreno-Mu{\~ n}oz, Pablo and Hauberg, S\oren
author:
- given: Stas
  family: Syrota
- given: Pablo
  family: Moreno-Mu\~ noz
- given: S\oren
  family: Hauberg
date: 2024-10-12
address:
container-title: Proceedings of the Geometry-grounded Representation Learning and
  Generative Modeling Workshop (GRaM)
volume: '251'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 10
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v251/main/assets/syrota24a/syrota24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
