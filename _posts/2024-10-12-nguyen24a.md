---
title: Alignment of MPNNs and Graph Transformers
abstract: As the complexity of machine learning (ML) model architectures increases,
  it is important to understand to what degree simpler and more efficient architectures
  can align with their complex counterparts. In this paper, we investigate the degree
  to which a Message Passing Neural Network (MPNN) can operate similarly to a Graph
  Transformer. We do this by training an MPNN to align with the intermediate embeddings
  of a Relational Transformer (RT). Throughout this process, we explore variations
  of the standard MPNN and assess the impact of different components on the degree
  of alignment. Our findings suggest that an MPNN can align to RT and the most important
  components that affect the alignment are the MPNN’s permutation invariant aggregation
  function, virtual node and layer normalisation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nguyen24a
month: 0
tex_title: Alignment of MPNNs and Graph Transformers
firstpage: 35
lastpage: 49
page: 35-49
order: 35
cycles: false
bibtex_author: Nguyen, Bao and Yodaiken, Anjana and Veli\v{c}kovi\'{c}, Petar
author:
- given: Bao
  family: Nguyen
- given: Anjana
  family: Yodaiken
- given: Petar
  family: Veličković
date: 2024-10-12
address:
container-title: Proceedings of the Geometry-grounded Representation Learning and
  Generative Modeling Workshop (GRaM)
volume: '251'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 10
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v251/main/assets/nguyen24a/nguyen24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
